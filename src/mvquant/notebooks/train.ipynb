{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from .dataloader.dataloader import DatasetCustom\n",
    "from .models import DLinear, NLinear, PatchTST\n",
    "from .models.stat_models import Arima\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from dataloader.api import *\n",
    "from pandas.tseries.offsets import BusinessDay\n",
    "import plotly.graph_objects as go\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "data = json.load(open('../datasets/data.json'))\n",
    "\n",
    "df = pd.DataFrame(data[\"data\"])\n",
    "cols = ['rowDate',\n",
    "    'volumeRaw',\n",
    "    'last_closeRaw',\n",
    "    'last_openRaw',\n",
    "    'last_maxRaw',\n",
    "    'last_minRaw',\n",
    "    'change_precentRaw']\n",
    "data = df[cols].copy()\n",
    "data.rowDate = pd.to_datetime(data.rowDate, format=\"%d/%m/%Y\")\n",
    "data.loc[:, \"volumeRaw\":] = data.loc[:, \"volumeRaw\":].astype(\"Float32\")\n",
    "print(data.info())\n",
    "data.head()\n",
    "data = data.sort_values(\"rowDate\", ascending=True).reset_index(drop=True)\n",
    "# data.to_parquet(\"./datasets/VNINDEX_2013-01-02_2023-09-11.parquet\", index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"VNINDEX\"\n",
    "# data_path = f\"{symbol}_2000-07-28_09-11-2023.parquet\"\n",
    "data_path = f\"{symbol}_2013-01-02_2023-09-11.parquet\"\n",
    "# data = get_historical_price_fireant(symbol, \"01-01-2000\", \"09-11-2023\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(f\"./datasets/{data_path}\")\n",
    "# data.rowDate = pd.to_datetime(data.rowDate, format=\"%Y/%m/%d\").dt.strftime(\"%Y/%m/%d\")\n",
    "# data.rowDate = pd.to_datetime(data.rowDate, format=\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "pm.autocorr_plot(data.last_closeRaw);\n",
    "pm.plot_pacf(data.last_closeRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    task_name=\"short_term_forecast\",\n",
    "    batch_size=512,\n",
    "    is_training=1,\n",
    "    seq_len=96, # 96  36 48 60 72 144 288\n",
    "    pred_len=96, # 96 192 336\n",
    "    label_len=0, # 48, 18\n",
    "    seasonal_patterns=None, # not used in this dataset\n",
    "    moving_avg=25, # 25 default\n",
    "    embed=\"timeF\",\n",
    "    activation=\"gelu\",\n",
    "    output_attention=False,\n",
    "    learning_rate=0.001,\n",
    "    epoch=30,\n",
    "    freq=\"D\",\n",
    "    enc_in=1, # number of features, 1 in case forecasting\n",
    "    num_class=1, # in case classification\n",
    "    individual=True,\n",
    "    scale=True,\n",
    "    time_enc=1,\n",
    "    features=\"S\", # [\"S\", \"M\", \"MS\"]\n",
    "    # transformer model\n",
    "    d_model=16,\n",
    "    e_layers=3,\n",
    "    d_ff=128,\n",
    "    c_out=1,\n",
    "    factor=3,\n",
    "    n_heads=4,\n",
    "    dropout=0.1, # 0.3 for patchTST\n",
    "    # fc_dropout=0.3, # dont use\n",
    "    head_dropout=0,\n",
    "    patch_len = 16,\n",
    "    stride = 8\n",
    ")\n",
    "\n",
    "time_configs = SimpleNamespace(\n",
    "    task_name=\"short_term_forecast\",\n",
    "    batch_size=256,\n",
    "    is_training=1,\n",
    "    seq_len=24, # 96  36 48 60 72 144 288\n",
    "    pred_len=24, # 96 192 336\n",
    "    label_len=0, # 48, 18\n",
    "    epoch=100,\n",
    "    freq=\"D\",\n",
    "    enc_in=1, # number of features, 1 in case forecasting\n",
    "    num_class=1, # in case classification\n",
    "    individual=True,\n",
    "    scale=True,\n",
    "    time_enc=1,\n",
    "    features=\"S\", # [\"S\", \"M\", \"MS\"]\n",
    "    sample=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = (0.95, 0.05)\n",
    "train_dataset = Dataset_Custom(\n",
    "    \"datasets\", \n",
    "    flag=\"train\",\n",
    "    features=configs.features, \n",
    "    data_path=data_path, \n",
    "    target=\"last_closeRaw\", \n",
    "    timeenc=configs.time_enc, \n",
    "    freq=configs.freq,\n",
    "    scale=configs.scale,\n",
    "    size=(configs.seq_len, configs.label_len, configs.pred_len),\n",
    "    train_test_ratio = train_test_ratio,\n",
    "    # train_only=True\n",
    ")\n",
    "val_dataset = Dataset_Custom(\n",
    "    \"datasets\", \n",
    "    flag=\"val\",\n",
    "    features=configs.features, \n",
    "    data_path=data_path, \n",
    "    target=\"last_closeRaw\", \n",
    "    timeenc=configs.time_enc, \n",
    "    freq=configs.freq,\n",
    "    scale=configs.scale,\n",
    "    size=(configs.seq_len, configs.label_len, configs.pred_len),\n",
    "    train_test_ratio = train_test_ratio\n",
    ")\n",
    "test_dataset = Dataset_Custom(\n",
    "    \"datasets\", \n",
    "    flag=\"test\",\n",
    "    features=configs.features, \n",
    "    data_path=data_path, \n",
    "    target=\"last_closeRaw\", \n",
    "    timeenc=configs.time_enc, \n",
    "    freq=configs.freq,\n",
    "    scale=configs.scale,\n",
    "    size=(configs.seq_len, configs.label_len, configs.pred_len),\n",
    "    train_test_ratio = train_test_ratio\n",
    ")\n",
    "\n",
    "train_dataloader= DataLoader(train_dataset, batch_size=configs.batch_size, drop_last=True, shuffle=True)\n",
    "val_dataloader= DataLoader(val_dataset, batch_size=configs.batch_size, drop_last=False, shuffle=False)\n",
    "test_dataloader= DataLoader(test_dataset, batch_size=configs.batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "\n",
    "def get_model(name, configs):\n",
    "    models = dict(\n",
    "        dlinear = DLinear(configs),\n",
    "        nlinear = NLinear(configs),\n",
    "        patchtst = PatchTST(configs),\n",
    "        arima = Arima(time_configs)\n",
    "    )\n",
    "    return models[name]\n",
    "model = get_model(\"dlinear\", configs)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.5)\n",
    "best_state = {}\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "f_dim = 0 if configs.features==\"S\" else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "for epoch in range(configs.epoch):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in train_dataloader:\n",
    "        seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        seq_x = seq_x.to(torch.float32)\n",
    "        seq_y = seq_y.to(torch.float32)\n",
    "        pred = model(seq_x, seq_x_mark, seq_y, seq_y_mark)[:, -configs.pred_len:, f_dim:]\n",
    "        seq_y = seq_y[:, -configs.pred_len:, f_dim:]\n",
    "        loss = torch.nn.functional.mse_loss(pred, seq_y)\n",
    "        train_losses.append(loss.detach().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"{epoch=} {np.mean(train_losses)=:.5f}\")\n",
    "    if len(test_dataset) > 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "            test_forecast = []\n",
    "            for batch in test_dataloader:\n",
    "                seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n",
    "                seq_x = seq_x.to(torch.float32)\n",
    "                seq_y = seq_y.to(torch.float32)\n",
    "                pred = model(seq_x, seq_x_mark, seq_y, seq_y_mark)[:, -configs.pred_len:, f_dim:]\n",
    "                seq_y = seq_y[:, -configs.pred_len:, f_dim:]\n",
    "                loss = torch.nn.functional.mse_loss(pred, seq_y).item()\n",
    "                val_loss.append(loss)\n",
    "                test_forecast.append(pred[0,:,:])\n",
    "            val_loss = np.mean(val_loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_epoch = epoch\n",
    "                best_val_loss = val_loss\n",
    "                best_state = deepcopy(model.state_dict())\n",
    "            train_loss = np.mean(train_losses)\n",
    "            print(f\"{epoch=} {train_loss=:.5f} {val_loss=:.5f} {best_val_loss=:.5f} {best_epoch=}\"  , sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast = []\n",
    "if len(test_dataset) > 0:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        test_forecast = []\n",
    "        for batch in test_dataloader:\n",
    "            seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n",
    "            seq_x = seq_x.to(torch.float32)\n",
    "            seq_y = seq_y.to(torch.float32)\n",
    "            pred = model(seq_x, seq_x_mark, seq_y, seq_y_mark)[:, -configs.pred_len:, f_dim:]\n",
    "            seq_y = seq_y[:, -configs.pred_len:, f_dim:]\n",
    "            loss = torch.nn.functional.mse_loss(pred, seq_y).item()\n",
    "            val_loss.append(loss)\n",
    "            test_forecast.append(pred[0,:,:])\n",
    "        val_loss = np.mean(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "        train_loss = np.mean(train_losses)\n",
    "        print(f\"{epoch=} {train_loss=:.5f} {val_loss=:.5f} {best_val_loss=:.5f} {best_epoch=}\"  , sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "plt.plot(train_dataset.inverse_transform(pred[0].numpy()))\n",
    "plt.plot(train_dataset.inverse_transform(test_dataset.data_y[-configs.seq_len:][0:configs.pred_len]))\n",
    "mean_squared_error(pred[0].numpy(), test_dataset.data_y[-configs.seq_len:][0:configs.pred_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"./models/checkpoints/model_{symbol}_dlinear_96_96_full.pth\")\n",
    "model = DLinear(configs)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "# model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =  train_dataset.scaler\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    conformal_scores = []\n",
    "    for batch in test_dataloader:\n",
    "        seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n",
    "        seq_x = seq_x.to(torch.float32)\n",
    "        seq_y = seq_y.to(torch.float32)\n",
    "        pred = model(seq_x, seq_x_mark, seq_y, seq_y_mark)[:, -configs.pred_len:, f_dim:]\n",
    "        seq_y = seq_y[:, -configs.pred_len:, f_dim:]\n",
    "        loss = torch.nn.functional.mse_loss(pred, seq_y)\n",
    "        val_losses.append(loss.item())\n",
    "        pred = scaler.inverse_transform(pred.numpy().reshape(1,-1))[0]\n",
    "        seq_y = scaler.inverse_transform(seq_y.numpy().reshape(1,-1))[0]\n",
    "        conformal_scores.append(abs(pred - seq_y)/pred)\n",
    "    print(f\"test_mse_loss:{np.mean(val_losses):.5f}\", sep=\"\\n\")\n",
    "pi95 = np.quantile(np.concatenate(conformal_scores), q=0.95)\n",
    "pi80 = np.quantile(np.concatenate(conformal_scores), q=0.8)\n",
    "print(pi95, np.concatenate(conformal_scores).max(), np.concatenate(conformal_scores).min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arima_dataset = Dataset_Custom(\n",
    "    \"datasets\", \n",
    "    flag=\"train\",\n",
    "    features=configs.features, \n",
    "    data_path=data_path, \n",
    "    target=\"last_closeRaw\", \n",
    "    timeenc=configs.time_enc, \n",
    "    freq=configs.freq,\n",
    "    scale=configs.scale,\n",
    "    size=(configs.seq_len, configs.label_len, configs.pred_len),\n",
    "    train_test_ratio = train_test_ratio,\n",
    "    train_only=True\n",
    ")\n",
    "# train_dataset.data_x = train_dataset.data_x[0, int(0.8)] \n",
    "# test_dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import ndiffs\n",
    "\n",
    "kpss_diffs = ndiffs(train_dataset.data_x, alpha=0.05, test=\"kpss\", max_d=6)\n",
    "adf_diffs = ndiffs(train_dataset.data_x, alpha=0.05, test=\"adf\", max_d=6)\n",
    "n_diffs = max(adf_diffs, kpss_diffs)\n",
    "\n",
    "print(f\"Estimated differencing term: {n_diffs}\")\n",
    "# Estimated differencing term: 1\n",
    "arima_model = pm.auto_arima(\n",
    "    train_dataset.data_x,\n",
    "    d=n_diffs,\n",
    "    seasonal=True,\n",
    "    stepwise=True,\n",
    "    m=10,\n",
    "    suppress_warnings=True,\n",
    "    error_action=\"ignore\",\n",
    "    max_p=6,\n",
    "    max_order=None,\n",
    "    trace=True,\n",
    ")\n",
    "print(arima_model.order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from pmdarima.metrics import smape\n",
    "\n",
    "def forecast_n_step(step=96):\n",
    "    fc, conf_int = arima_model.predict(n_periods=step, return_conf_int=True)\n",
    "    return (\n",
    "        fc,\n",
    "        np.asarray(conf_int).tolist())\n",
    "arima_val, val_conf_int = forecast_n_step(len(val_dataset.data_x))\n",
    "print(mean_squared_error(val_dataset.data_x, arima_val))\n",
    "\n",
    "# forecasts = []\n",
    "# confidence_intervals = []\n",
    "# arima_losses = []\n",
    "# for i, new_ob in enumerate(test_dataset.data_x[:-time_configs.pred_len]):\n",
    "#     fc, conf = forecast_n_step(time_configs.pred_len)\n",
    "#     confidence_intervals.append(conf)\n",
    "#     # Updates the existing model with a small number of MLE steps\n",
    "#     # because we dont have new_ob\n",
    "#     loss = mean_squared_error(test_dataset.data_x[i:i+96], fc)\n",
    "#     arima_losses.append(loss)\n",
    "#     arima_model.update(new_ob)\n",
    "\n",
    "# print(f\"SMAPE: {smape(test_dataset.data_x, forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_dataset.inverse_transform(val_dataset.data_x).reshape(-1))\n",
    "plt.plot(train_dataset.inverse_transform(arima_val.reshape(-1,1)).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, new_ob in enumerate(test_dataset.data_x[-time_configs.pred_len:]):\n",
    "    arima_model.update(new_ob)\n",
    "scaler.inverse_transform(test_dataset.data_x[-time_configs.pred_len:]).reshape(-1)\n",
    "fc, conf = forecast_n_step(time_configs.pred_len)\n",
    "arima_forcast = scaler.inverse_transform(fc.reshape(-1,1))\n",
    "arima_forcast.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from dataloader.api import get_historical_price_vnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = timezone(timedelta(hours=7))\n",
    "start_time = int((data.rowDate.max() + BusinessDay(1)).timestamp())\n",
    "end_time = int(datetime.now(tz=tz).timestamp())\n",
    "forecast_gt_df = get_historical_price_vnd(symbol=\"VNINDEX\", start_time=start_time, end_time=end_time)\n",
    "forecast_gt_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_seq_x, pred_seq_y, pred_seq_x_mark, pred_seq_y_mark = test_dataset[len(test_dataset.data_x) - configs.seq_len]\n",
    "    pred_seq_x = torch.as_tensor(pred_seq_x)[None, :].to(torch.float32)\n",
    "    pred_seq_y = torch.as_tensor(pred_seq_y)[None, :].to(torch.float32)\n",
    "    pred_new = model(pred_seq_x, pred_seq_x_mark, pred_seq_y, pred_seq_y_mark)[:, -configs.pred_len:, 0:]\n",
    "    print(pred_seq_x.shape, pred_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_days = np.round(test_dataset.inverse_transform(pred_seq_x.numpy().reshape(1,-1))[0],2)\n",
    "next_days = np.round(test_dataset.inverse_transform(pred_new.numpy().reshape(1,-1))[0],2)\n",
    "next_days_pi95_low = np.round(next_days*(1-pi95),2)\n",
    "next_days_pi95_high = np.round(next_days*(1+pi95),2) \n",
    "next_days_pi80_low = np.round(next_days*(1-pi80),2) \n",
    "next_days_pi80_high = np.round(next_days*(1+pi80),2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi95, pi80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holiday = pd.read_csv(\"./datasets/vn_holiday_2025.csv\")\n",
    "df_holiday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = data.rowDate.max() + BusinessDay(1)\n",
    "pred_dates = pd.bdate_range(last_date, pd.Timestamp(last_date) + BusinessDay(configs.pred_len+ 50)) # add more 50 days due to holiday offline\n",
    "pred_dates = list(filter(lambda x: x not in df_holiday[\"Date\"].tolist(), pred_dates))\n",
    "pred_dates = pred_dates[:configs.pred_len]\n",
    "data_visual = data.query(\"rowDate >= '2022/01/01'\")\n",
    "\n",
    "colors = {\n",
    "    \"train\": \"rgb(48, 133, 195)\",\n",
    "    \"forecast\": \"rgb(233, 184, 36)\",\n",
    "    \"actual\": \"rgb(33, 156, 144)\",\n",
    "    \"fill95\": \"rgba(255, 217, 61,0.1)\",\n",
    "    \"fill80\": \"rgba(255, 217, 61,0.15)\",\n",
    "}\n",
    "\n",
    "template=\"plotly_dark\"\n",
    "plot_data = []\n",
    "\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=data_visual.rowDate,\n",
    "        y=data_visual.last_closeRaw,\n",
    "        name=\"Training\",\n",
    "        legendrank=1,\n",
    "        line=dict(color=colors[\"train\"], width=2),\n",
    "    )\n",
    ")\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=pred_dates,\n",
    "        y=next_days,\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=colors[\"forecast\"], width=2),\n",
    "        legendrank=2\n",
    "    )\n",
    ")\n",
    "\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=forecast_gt_df.rowDate,\n",
    "        y=forecast_gt_df.last_closeRaw,\n",
    "        name=\"Actual\",\n",
    "        line=dict(color=colors[\"actual\"], width=2),\n",
    "        legendrank=3,\n",
    "        mode=\"lines\",\n",
    "    )\n",
    ")\n",
    "\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=pred_dates, \n",
    "        y=next_days_pi95_high,\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"), \n",
    "        showlegend=False,\n",
    "        hoverinfo=\"skip\"\n",
    "    )\n",
    ")\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=pred_dates,\n",
    "        y=next_days_pi95_low,\n",
    "        name=\"Predict Interval 95th\",\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=colors[\"fill95\"],\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        hoverinfo=\"skip\",\n",
    "    )\n",
    ")\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=pred_dates, y=next_days_pi80_high, line=dict(color=\"rgba(0,0,0,0)\"), showlegend=False, hoverinfo=\"skip\"\n",
    "    )\n",
    ")\n",
    "plot_data.append(\n",
    "    go.Scatter(\n",
    "        x=pred_dates,\n",
    "        y=next_days_pi80_low,\n",
    "        name=\"Predict Interval 80th\",\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=colors[\"fill80\"],\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        hoverinfo=\"skip\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "layout = dict(\n",
    "    title=dict(text=f\"{symbol} LONGTERM FORECASTING\"),\n",
    "    hovermode=\"x unified\",\n",
    "    font_size=13,\n",
    "    legend=dict(\n",
    "        borderwidth=2,\n",
    "    ),\n",
    "    template=template,\n",
    "    yaxis=dict(\n",
    "        title=symbol,\n",
    "        # linecolor=\"black\" if \"white\" in template else \"white\",\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        # linecolor=\"black\" if \"white\" in template else \"white\",\n",
    "        title=\"Date\",\n",
    "        rangeslider_visible=True,\n",
    "        rangeselector=dict(\n",
    "            buttons=list(\n",
    "                [\n",
    "                    dict(count=6, label=\"6M\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"1Y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(step=\"all\", label=\"ALL\"),\n",
    "                ]\n",
    "            ),\n",
    "            # font=dict(color=\"black\"),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=plot_data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     {\n",
    "#         \"model\": model.state_dict(),\n",
    "#         \"scaler\": train_dataset.scaler\n",
    "#     },\n",
    "#     f\"./models/checkpoints/model_{symbol}_dlinear_96_96_full.pth\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
